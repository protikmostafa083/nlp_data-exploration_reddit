# Reddit Data Exploration


## Overview

The Reddit Data Exploration project is a comprehensive analysis of burnout-related data collected from Reddit. The project aims to gain insights into the experiences, challenges, and sentiments shared by users within the Reddit community. By applying advanced Natural Language Processing (NLP) techniques, the project uncovers hidden patterns, identifies key themes, and provides a deeper understanding of burnout in various contexts.

## Key Features

- **Data Collection:** Collect burnout-related data from Reddit using the powerful PRAW (Python Reddit API Wrapper) library.

- **Data Preprocessing:** Cleanse and prepare the raw data through various preprocessing steps such as handling missing values, removing duplicates, and transforming the text for analysis.

- **NLP Techniques:** Utilize cutting-edge NLP techniques to extract meaningful information from the data, including word cloud generation, collocation analysis, named entity recognition, and more.

- **Topic Modeling:** Employ Latent Dirichlet Allocation (LDA) to discover latent topics within the dataset and uncover the most prevalent themes related to burnout.

- **Sentiment Analysis:** Evaluate the emotional tone of the posts and assess the overall sentiment around burnout, providing valuable insights into users' experiences.

- **Summarization:** Generate concise summaries of lengthy text data, enabling quick understanding and extraction of essential information.

- **Visualization:** Create intuitive visualizations, including charts, word clouds, and tables, to present the findings in a visually appealing and accessible manner.

## Technologies Used

- Python: The primary programming language used for data analysis, NLP processing, and application development.

- Streamlit: The web application framework utilized for building an interactive and user-friendly interface.

- Spacy and NLTK: Industry-standard libraries for natural language processing tasks, including tokenization, lemmatization, and named entity recognition.

- Pandas: A versatile data manipulation library for efficient data handling and analysis.

- GitHub: A reliable version control platform for collaboration and project management.

## Installation and Usage

1. Clone the GitHub repository:

   ```shell
   git clone https://github.com/protikmostafa083/nlp_data-exploration_reddit.git

2. Install the required dependencies:
   ```shell
   pip install -e .

## Usage

1. Navigate to the project directory:

   ```shell
   cd repo-name
   
2. Run the Streamlit application::
   ```shell
   streamlit run main.py

3. Access the web application in your local browser:
    ```shell
   http://localhost:8501

## Contribution Guidelines
Contributions to the project are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.

## License
This project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike [(CC BY-NC-SA)]((https://creativecommons.org/licenses/by-nc-sa/)) license

## Contact
For any inquiries or further information, please feel free to reach out to me at mostafa.m.jalal@student.uts.edu.au. We'd love to hear from you!